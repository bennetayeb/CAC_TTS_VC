{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N7fApNF7xjy3"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install scprep\n",
    "!pip install anndata\n",
    "!pip install scanpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-L0n8_rB7gPQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata\n",
    "import scprep\n",
    "import scanpy as sc\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tempfile\n",
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import load_raw\n",
    "import normalize_tools as nm\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIaWncv9FRlG"
   },
   "source": [
    "# **try out with scicar cell lines dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kdBrFDRFj-x"
   },
   "source": [
    "**1. URLs for raw data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           gene_type gene_short_name\n",
      "gene_id                                             \n",
      "ENSG00000223972.4         pseudogene         DDX11L1\n",
      "ENSG00000227232.4         pseudogene          WASH7P\n",
      "ENSG00000243485.2            lincRNA      MIR1302-11\n",
      "ENSG00000237613.2            lincRNA         FAM138A\n",
      "ENSG00000268020.2         pseudogene          OR4G4P\n",
      "...                              ...             ...\n",
      "ENSMUSG00000064368.1  protein_coding          mt-Nd6\n",
      "ENSMUSG00000064369.1         Mt_tRNA           mt-Te\n",
      "ENSMUSG00000064370.1  protein_coding         mt-Cytb\n",
      "ENSMUSG00000064371.1         Mt_tRNA           mt-Tt\n",
      "ENSMUSG00000064372.1         Mt_tRNA           mt-Tp\n",
      "\n",
      "[113153 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "rna_data, atac_data, rna_cells, atac_cells, rna_genes, atac_genes = load_raw.load_raw_cell_lines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vmgqCzKUTtpo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rania/PycharmProjects/multimodal/load_raw.py:125: FutureWarning: X.dtype being converted to np.float32 from float64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
      "  adata = anndata.AnnData(\n"
     ]
    }
   ],
   "source": [
    "scicar_data, joint_index, keep_cells_idx = load_raw.merge_data(rna_data, atac_data, rna_cells, atac_cells, rna_genes, atac_genes)\n",
    "#rna_df, atac_df = ann2df(scica|r_data)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_vae(\n",
    "    adata,\n",
    "    hvg_rna_prop=0.6,\n",
    "    hvg_atac_prop=0.3,\n",
    "):\n",
    "    #select highly variable genes respectively for RNA and ATAC modalities\n",
    "    nm.log_cpm(adata)\n",
    "    nm.log_cpm(adata, obsm = \"mode2\", obs = \"mode2_obs\", var = \"mode2_var\")\n",
    "    nm.hvg_by_sc(adata, proportion = hvg_rna_prop)\n",
    "    nm.hvg_by_sc(\n",
    "        adata, \n",
    "        obsm = \"mode2\", \n",
    "        obs = \"mode2_obs\", \n",
    "        var = \"mode2_var\", \n",
    "        proportion = hvg_atac_prop,\n",
    "    )\n",
    "    adata.uns[\"mode2_obs\"] = np.array(adata.uns[\"mode2_obs\"][0])\n",
    "    adata.uns[\"mode2_var\"] = np.array(adata.uns[\"mode2_var\"][0])\n",
    "    dim_mode1 = adata.X.shape[1]\n",
    "    dim_mode2 = adata.obsm[\"mode2\"].shape[1]\n",
    "    return adata, dim_mode1, dim_mode2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rania/PycharmProjects/multimodal/normalize_tools.py:38: FutureWarning: X.dtype being converted to np.float32 from float64. In the next version of anndata (0.9) conversion will not be automatic. Pass dtype explicitly to avoid this warning. Pass `AnnData(X, dtype=X.dtype, ...)` to get the future behavour.\n",
      "  adata_temp = anndata.AnnData(adata.obsm[obsm], obs = obs, var = var)\n"
     ]
    }
   ],
   "source": [
    "train_data_raw, test_data_raw, indices_train, mask_test = load_raw.train_test_split(scicar_data)\n",
    "scicar_data_filtered, dim_rna, dim_atac = preprocess_vae(scicar_data)\n",
    "train_data_filtered, test_data_filtered = load_raw.split_with_mask(scicar_data_filtered, indices_train, mask_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up all hyper-parameters\n",
    "hyper = {\n",
    "    \"nEpochs\":100,\n",
    "    \"dimRNA\":dim_rna,\n",
    "    \"dimATAC\":dim_atac,\n",
    "    \"layer_sizes\":[1024, 512, 256],\n",
    "    \"nz\":128,\n",
    "    \"batchSize\":512,\n",
    "    \"lr\":1e-3,\n",
    "    \"add_hinge\":True,\n",
    "    \"lamb_hinge\":10,\n",
    "    \"lamb_match\":1,\n",
    "    \"lamb_nn\":1.5,\n",
    "    \"lamb_kl\":1e-9,\n",
    "    \"lamb_anc\":1e-9,\n",
    "    \"clip_grad\":0.1,\n",
    "    \"checkpoint_path\": './checkpoint/vae_hinge.pt',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hD2gQCx6iIlc"
   },
   "source": [
    "# **define pytorch datasets for RNA and ATAC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6qQdzukDtFhu"
   },
   "outputs": [],
   "source": [
    "class Merge_Dataset(Dataset):\n",
    "    def __init__(self, adata_raw, adata_filtered):\n",
    "        self.rna_data_filtered, self.atac_data_filtered = self._load_merge_data(adata_filtered)\n",
    "        self.rna_data_raw = self._load_raw_ref_data(adata_raw)\n",
    "        \n",
    "    def __len__(self):\n",
    "        #assert(len(self.rna_data) == len(self.atac_data))\n",
    "        return len(self.atac_data_filtered)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        rna_filtered = self.rna_data_filtered.values[idx]\n",
    "        atac_filtered = self.atac_data_filtered.values[idx]\n",
    "        rna_raw = self.rna_data_raw.values[idx]\n",
    "        #return a tensor that for a single observation\n",
    "        return [\n",
    "            torch.from_numpy(rna_filtered).float(),\n",
    "            torch.from_numpy(atac_filtered).float(),\n",
    "            torch.from_numpy(rna_raw).float(),\n",
    "        ]\n",
    "  \n",
    "    def _load_merge_data(self, adata):\n",
    "        rna_df = pd.DataFrame(\n",
    "            data = adata.X.toarray(), \n",
    "            index = np.array(adata.obs.index), \n",
    "            columns = np.array(adata.var.index),\n",
    "        )\n",
    "        atac_df = pd.DataFrame(\n",
    "            data = adata.obsm[\"mode2\"].toarray(), \n",
    "            index = np.array(adata.uns[\"mode2_obs\"]), \n",
    "            columns = np.array(adata.uns[\"mode2_var\"]),\n",
    "        )\n",
    "        return rna_df, atac_df\n",
    "    \n",
    "    def _load_raw_ref_data(self, adata_raw):\n",
    "        rna_df = pd.DataFrame(\n",
    "            data = adata_raw.X.toarray(), \n",
    "            index = np.array(adata_raw.obs.index), \n",
    "            columns = np.array(adata_raw.var.index),\n",
    "        )\n",
    "        return rna_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUl-7w_gcmto"
   },
   "source": [
    "# **define basic models(autoencoders) for learning latent space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "46TuWH_Rgwnc"
   },
   "outputs": [],
   "source": [
    "class FC_VAE(nn.Module):\n",
    "    def __init__(self, n_input, nz, layer_sizes=hyper[\"layer_sizes\"]):\n",
    "        super(FC_VAE, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.nz = nz\n",
    "        self.layer_sizes = layer_sizes\n",
    "\n",
    "        self.encoder_layers = []\n",
    "\n",
    "        self.encoder_layers.append(nn.Linear(n_input, self.layer_sizes[0]))\n",
    "        self.encoder_layers.append(nn.LeakyReLU(inplace=True))\n",
    "        self.encoder_layers.append(nn.BatchNorm1d(self.layer_sizes[0]))\n",
    "\n",
    "        for layer_idx in range(len(layer_sizes)-1):\n",
    "            if layer_idx == len(layer_sizes) - 2:\n",
    "                self.encoder_layers.append(nn.Linear(self.layer_sizes[layer_idx], self.layer_sizes[layer_idx+1]))\n",
    "                self.encoder_layers.append(nn.LeakyReLU(inplace=True))\n",
    "            else:\n",
    "                self.encoder_layers.append(nn.Linear(self.layer_sizes[layer_idx], self.layer_sizes[layer_idx+1]))\n",
    "                self.encoder_layers.append(nn.BatchNorm1d(self.layer_sizes[layer_idx+1]))\n",
    "                self.encoder_layers.append(nn.LeakyReLU(inplace=True))\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            *self.encoder_layers\n",
    "        )\n",
    "        self.fc1 = nn.Linear(self.layer_sizes[-1], nz)\n",
    "        self.fc2 = nn.Linear(self.layer_sizes[-1], nz)\n",
    "\n",
    "        self.decoder_layers = []\n",
    "        self.decoder_layers.append(nn.Linear(nz, self.layer_sizes[-1]))\n",
    "        self.decoder_layers.append(nn.BatchNorm1d(self.layer_sizes[-1]))\n",
    "        self.decoder_layers.append(nn.LeakyReLU(inplace=True))\n",
    "\n",
    "        for layer_idx in range(len(self.layer_sizes)-1, 0, -1):\n",
    "            self.decoder_layers.append(nn.Linear(self.layer_sizes[layer_idx], self.layer_sizes[layer_idx-1]))\n",
    "            self.decoder_layers.append(nn.BatchNorm1d(self.layer_sizes[layer_idx-1]))\n",
    "            self.decoder_layers.append(nn.LeakyReLU(inplace=True))\n",
    "\n",
    "        self.decoder_layers.append(nn.Linear(self.layer_sizes[0], self.n_input))\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            *self.decoder_layers\n",
    "        )\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc1(h), self.fc2(h)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        #calculate std from log(var)\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        res = self.decode(z)\n",
    "        return res, z, mu, logvar\n",
    "\n",
    "    def get_latent_var(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return z\n",
    "    \n",
    "    def generate(self, z):\n",
    "        return self.decode(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htBAUHFgqT7B"
   },
   "source": [
    "# **train VAE model based on reconstruction, KL divergence, and anchor loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    n_svd = 100\n",
    "    proportion_neighbors = 0.1\n",
    "    \n",
    "    rna_inputs_filtered, atac_inputs_filtered, rna_inputs_raw = zip(*batch)\n",
    "    rna_inputs_filtered = torch.stack(rna_inputs_filtered)\n",
    "    atac_inputs_filtered = torch.stack(atac_inputs_filtered)\n",
    "    rna_inputs_raw = torch.stack(rna_inputs_raw)\n",
    "    \n",
    "    n_svd = min([n_svd, min(rna_inputs_raw.shape) - 1])\n",
    "    n_neighbors = int(np.ceil(proportion_neighbors * rna_inputs_raw.shape[0]))\n",
    "    X_pca = sklearn.decomposition.TruncatedSVD(n_svd).fit_transform(rna_inputs_raw)\n",
    "    _, indices_true = (\n",
    "        sklearn.neighbors.NearestNeighbors(n_neighbors=n_neighbors).fit(X_pca).kneighbors(X_pca)\n",
    "    )\n",
    "    \n",
    "    return rna_inputs_filtered, atac_inputs_filtered, rna_inputs_raw, torch.from_numpy(indices_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ToOEhyd6m-ZB"
   },
   "outputs": [],
   "source": [
    "#load dataset and split train and test data\n",
    "def get_data_loaders(train_data_filtered, test_data_filtered, train_data_raw, test_data_raw):\n",
    "    train_set = Merge_Dataset(train_data_raw, train_data_filtered)\n",
    "    test_set = Merge_Dataset(test_data_raw, test_data_filtered)\n",
    "    #load data loader\n",
    "    train_loader = DataLoader(\n",
    "        train_set, \n",
    "        batch_size=hyper[\"batchSize\"], \n",
    "        collate_fn=collate_fn, \n",
    "        drop_last=False, \n",
    "        shuffle=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_set, \n",
    "        batch_size=test_data_filtered.shape[0], \n",
    "        collate_fn=collate_fn, \n",
    "        drop_last=False,\n",
    "        shuffle=False\n",
    "    )\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructureHingeLoss(nn.Module):\n",
    "    def __init__(self, margin, max_val, lamb_match, lamb_nn, device):\n",
    "        super(StructureHingeLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.max_val = max_val\n",
    "        self.lamb_match = lamb_match\n",
    "        self.lamb_nn = lamb_nn\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, rna_outputs, atac_outputs, nn_indices):\n",
    "        #rna_outputs: n_batch x n_latent\n",
    "        #atac_outputs: n_batch x n_latent\n",
    "        assert rna_outputs.shape[0] == atac_outputs.shape[0]\n",
    "        assert rna_outputs.shape[1] == atac_outputs.shape[1]\n",
    "        n_batch = rna_outputs.shape[0]\n",
    "        \n",
    "        #calculated pairwise L2 distance\n",
    "        #dist_rna_atac[i][j]: the L2 distance between RNA embedding i\n",
    "        #and ATAC embedding j (n_batch x n_batch)\n",
    "        #constraint for ensuring every rna embedding is close to matched atac embedding\n",
    "        dist_rna_atac = torch.cdist(rna_outputs, atac_outputs, p=2)\n",
    "        match_labels = torch.eye(n_batch).to(self.device)\n",
    "        match_mask = match_labels > 0\n",
    "        pos_match_dist = torch.masked_select(dist_rna_atac, match_mask).view(n_batch, 1)\n",
    "        neg_match_dist = torch.masked_select(dist_rna_atac, ~match_mask).view(n_batch, -1)\n",
    "        \n",
    "        loss_match_rna = torch.clamp(self.margin + pos_match_dist - neg_match_dist, 0, self.max_val)\n",
    "        loss_match_rna = loss_match_rna.mean()\n",
    "        #print(f\"loss_match_rna: {loss_match_rna}\")\n",
    "        \n",
    "        #constraint for ensuring every atac embedding is close to matched rna embedding\n",
    "        dist_atac_rna = dist_rna_atac.t()\n",
    "        pos_match_dist = torch.masked_select(dist_atac_rna, match_mask).view(n_batch, 1)\n",
    "        neg_match_dist = torch.masked_select(dist_atac_rna, ~match_mask).view(n_batch, -1)\n",
    "        \n",
    "        loss_match_atac = torch.clamp(self.margin + pos_match_dist - neg_match_dist, 0, self.max_val)\n",
    "        loss_match_atac = loss_match_rna.mean()\n",
    "        #print(f\"loss_match_atac: {loss_match_atac}\")\n",
    "        \n",
    "        #constraint for ensuring that every RNA embedding is close to \n",
    "        #the neighboring RNA embeddings.\n",
    "        nn_masked = torch.zeros(n_batch, n_batch).to(self.device)\n",
    "        nn_masked.scatter_(1, nn_indices, 1.)\n",
    "        nn_masked = nn_masked > 0\n",
    "        \n",
    "        dist_rna_rna = torch.cdist(rna_outputs, rna_outputs, p=2)\n",
    "        \n",
    "        #pos_rna_nn_dist: n_batch x n_neighbor\n",
    "        pos_rna_nn_dist = torch.masked_select(dist_rna_rna, nn_masked).view(n_batch, -1)\n",
    "        neg_rna_nn_dist = torch.masked_select(dist_rna_rna, ~nn_masked).view(n_batch, -1)\n",
    "        rna_nn_loss = torch.clamp(self.margin + pos_rna_nn_dist[...,None] - neg_rna_nn_dist[..., None, :], 0, self.max_val)\n",
    "        rna_nn_loss = rna_nn_loss.mean()\n",
    "        #print(f\"rna_nn_loss: {rna_nn_loss}\")\n",
    "        \n",
    "        #constraint for ensuring that every ATAC embedding is close to \n",
    "        #the neighboring ATAC embeddings.\n",
    "        dist_atac_atac = torch.cdist(atac_outputs, atac_outputs, p=2)\n",
    "        #pos_rna_nn_dist: n_batch x n_neighbor\n",
    "        pos_atac_nn_dist = torch.masked_select(dist_atac_atac, nn_masked).view(n_batch, -1)\n",
    "        neg_atac_nn_dist = torch.masked_select(dist_atac_atac, ~nn_masked).view(n_batch, -1)\n",
    "        atac_nn_loss = torch.clamp(self.margin + pos_atac_nn_dist[...,None] - neg_atac_nn_dist[..., None, :], 0, self.max_val)\n",
    "        atac_nn_loss = atac_nn_loss.mean()\n",
    "        #print(f\"atac_nn_loss: {atac_nn_loss}\")\n",
    "        \n",
    "        loss = (self.lamb_match * loss_match_rna \n",
    "                + self.lamb_match * loss_match_atac\n",
    "                + self.lamb_nn * rna_nn_loss \n",
    "                + self.lamb_nn * atac_nn_loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LH_uoNVQ6v1O"
   },
   "outputs": [],
   "source": [
    "#set up loss function\n",
    "def basic_loss(recon_x, x, mu, logvar, lamb1):\n",
    "    MSE = nn.MSELoss()\n",
    "    lloss = MSE(recon_x, x)\n",
    "    #KL divergence\n",
    "    KL_loss = -0.5*torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    lloss = lloss + lamb1*KL_loss\n",
    "    return lloss\n",
    "\n",
    "#anchor loss for minimizing distance between paired observation\n",
    "def anchor_loss(embed_rna, embed_atac):\n",
    "    L1 = nn.L2Loss()\n",
    "    anc_loss = L2(embed_rna, embed_atac)\n",
    "    return anc_loss\n",
    "\n",
    "def hinge_loss(\n",
    "    margin, \n",
    "    max_val, \n",
    "    lamb_match,\n",
    "    lamb_nn, \n",
    "    embed_rna, \n",
    "    embed_atac, \n",
    "    nn_indices,\n",
    "):\n",
    "    Hinge_Loss = StructureHingeLoss(margin, max_val, lamb_match, lamb_nn)\n",
    "    loss = Hinge_Loss(embed_rna, embed_atac, nn_indices)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_criteria(rna_inputs, rna_outputs, atac_outputs, proportion_neighbors=0.1, n_svd=100):\n",
    "    n_svd = min([n_svd, min(rna_inputs.shape)-1])\n",
    "    n_neighbors = int(np.ceil(proportion_neighbors*rna_inputs.shape[0]))\n",
    "    X_pca = sklearn.decomposition.TruncatedSVD(n_svd).fit_transform(rna_inputs)\n",
    "    _, indices_true = (\n",
    "        sklearn.neighbors.NearestNeighbors(n_neighbors = n_neighbors).fit(X_pca).kneighbors(X_pca)\n",
    "    )\n",
    "    _, indices_pred = (\n",
    "        sklearn.neighbors.NearestNeighbors(n_neighbors=n_neighbors).fit(rna_outputs).kneighbors(atac_outputs)\n",
    "    )\n",
    "    neighbors_match = np.zeros(n_neighbors, dtype=int)\n",
    "    for i in range(rna_inputs.shape[0]):\n",
    "        _, pred_matches, true_matches = np.intersect1d(\n",
    "            indices_pred[i], indices_true[i], return_indices=True\n",
    "        )\n",
    "        neighbors_match_idx = np.maximum(pred_matches, true_matches)\n",
    "        neighbors_match += np.sum(np.arange(n_neighbors) >= neighbors_match_idx[:, None], axis = 0,)\n",
    "    neighbors_match_curve = neighbors_match/(np.arange(1, n_neighbors + 1) * rna_inputs.shape[0])\n",
    "    area_under_curve = np.mean(neighbors_match_curve)\n",
    "    return area_under_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "q8CX7V2p_tNA"
   },
   "outputs": [],
   "source": [
    "#set up train functions\n",
    "def main():\n",
    "    #load training data and testing data\n",
    "    train_loader, test_loader = get_data_loaders(\n",
    "        train_data_filtered,\n",
    "        test_data_filtered,\n",
    "        train_data_raw,\n",
    "        test_data_raw,\n",
    "    )\n",
    "    \n",
    "    #load checkpoint\n",
    "    checkpoint = None\n",
    "    if path.exists(hyper[\"checkpoint_path\"]):\n",
    "        checkpoint = torch.load(hyper[\"checkpoint_path\"])\n",
    "    \n",
    "    #load basic models\n",
    "    netRNA = FC_VAE(n_input=hyper[\"dimRNA\"], nz=hyper[\"nz\"], layer_sizes=hyper[\"layer_sizes\"])\n",
    "    netATAC = FC_VAE(n_input=hyper[\"dimATAC\"], nz=hyper[\"nz\"], layer_sizes=hyper[\"layer_sizes\"])\n",
    "    if checkpoint != None:\n",
    "        netRNA.load_state_dict(checkpoint[\"net_rna_state_dict\"])\n",
    "        netATAC.load_state_dict(checkpoint[\"net_atac_state_dict\"])\n",
    "        \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"using GPU\")\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    netRNA.to(device)\n",
    "    netATAC.to(device)\n",
    "    \n",
    "    #setup optimizers for two nets\n",
    "    opt_netRNA = optim.Adam(list(netRNA.parameters()), lr=hyper[\"lr\"])\n",
    "    opt_netATAC = optim.Adam(list(netATAC.parameters()), lr=hyper[\"lr\"])\n",
    "    scheduler_netRNA = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        opt_netRNA,\n",
    "        patience=10,\n",
    "        threshold=0.01,\n",
    "        threshold_mode=\"abs\",\n",
    "        min_lr=1e-5,\n",
    "    )\n",
    "    scheduler_netATAC = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        opt_netATAC,\n",
    "        patience=10,\n",
    "        threshold=0.01,\n",
    "        threshold_mode=\"abs\",\n",
    "        min_lr=1e-5,\n",
    "    )\n",
    "    \n",
    "    best_knn_auc = 0\n",
    "    if checkpoint != None:\n",
    "        best_knn_auc = checkpoint[\"dev_acc\"]\n",
    "        \n",
    "    #training\n",
    "    for epoch in range(hyper[\"nEpochs\"]):\n",
    "        train_losses = []\n",
    "        #train for epochs\n",
    "        for idx, (rna_inputs_filtered, atac_inputs_filtered, rna_inputs_raw, nn_indices) in enumerate(train_loader):\n",
    "            rna_inputs_filtered = Variable(rna_inputs_filtered).to(device)\n",
    "            atac_inputs_filtered = Variable(atac_inputs_filtered).to(device)\n",
    "            rna_inputs_raw = Variable(rna_inputs_raw).to(device)\n",
    "            nn_indices = Variable(nn_indices).to(device)\n",
    "            \n",
    "            opt_netATAC.zero_grad()\n",
    "            opt_netRNA.zero_grad()\n",
    "            recon_rna, z_rna, mu_rna, logvar_rna = netRNA(rna_inputs_filtered)\n",
    "            recon_atac, z_atac, mu_atac, logvar_atac = netATAC(atac_inputs_filtered)\n",
    "            rna_loss = basic_loss(recon_rna, rna_inputs_filtered, mu_rna, logvar_rna, lamb1=hyper[\"lamb_kl\"])\n",
    "            atac_loss = basic_loss(recon_atac, atac_inputs_filtered, mu_atac, logvar_atac, lamb1=hyper[\"lamb_kl\"])\n",
    "            \n",
    "            if hyper[\"add_hinge\"]:\n",
    "                hinge_loss = StructureHingeLoss(\n",
    "                    margin=0.3, \n",
    "                    max_val=1e6, \n",
    "                    lamb_match=hyper[\"lamb_match\"], \n",
    "                    lamb_nn=hyper[\"lamb_nn\"],\n",
    "                    device=device,\n",
    "                )\n",
    "                h_loss = hinge_loss(z_rna, z_atac, nn_indices)\n",
    "            '''if epoch % 5 == 0:\n",
    "                print(f\"rna_loss: {rna_loss}\")\n",
    "                print(f\"atac_loss:{atac_loss}\")\n",
    "                print(f\"anc_loss: {anc_loss}\")\n",
    "                print(f\"hinge loss: {h_loss}\")'''\n",
    "\n",
    "            #loss functions for each modalities\n",
    "            train_loss = rna_loss + atac_loss + hyper[\"lamb_hinge\"] * h_loss\n",
    "            #train_loss = rna_loss + atac_loss\n",
    "            #train_loss = rna_loss + atac_loss + hyper[\"lamb_anc\"] * anc_loss\n",
    "            #rain_loss = rna_loss + atac_loss + hyper[\"lamb_anc\"] * anc_loss + h_loss\n",
    "            train_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(netRNA.parameters(), max_norm=hyper[\"clip_grad\"])\n",
    "            nn.utils.clip_grad_norm_(netATAC.parameters(), max_norm=hyper[\"clip_grad\"])\n",
    "            opt_netRNA.step()\n",
    "            opt_netATAC.step()\n",
    "            train_losses.append(train_loss.item())\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch: \" + str(epoch) + \", train loss: \" + str(avg_train_loss))\n",
    "        \n",
    "        #evaluating step\n",
    "        with torch.no_grad():\n",
    "            netRNA.eval()\n",
    "            netATAC.eval()\n",
    "            knn_acc = []\n",
    "            for idx, samples in enumerate(test_loader):\n",
    "                rna_inputs_filtered = samples[0].float().to(device)\n",
    "                atac_inputs_filtered = samples[1].float().to(device)\n",
    "                rna_inputs_raw = samples[2].float().to(device)\n",
    "\n",
    "                _, output_rna, _, _ = netRNA(rna_inputs_filtered)\n",
    "                _, output_atac, _, _ = netATAC(atac_inputs_filtered)\n",
    "                knn_acc.append(knn_criteria(rna_inputs_raw.cpu().detach(), output_rna.cpu().detach(), output_atac.cpu().detach()))\n",
    "            avg_knn_auc = np.mean(knn_acc)\n",
    "            if avg_knn_auc > best_knn_auc:\n",
    "                torch.save({\n",
    "                    \"epoch\":epoch,\n",
    "                    \"lamb_match\":hyper[\"lamb_match\"],\n",
    "                    \"lamb_nn\":hyper[\"lamb_nn\"],\n",
    "                    \"lamb_hinge\":hyper[\"lamb_hinge\"],\n",
    "                    \"clip_grad\":hyper['clip_grad'],\n",
    "                    \"layer_sizes\":hyper['layer_sizes'],\n",
    "                    \"lr\": hyper[\"lr\"],\n",
    "                    \"net_rna_state_dict\":netRNA.state_dict(),\n",
    "                    \"net_atac_state_dict\":netATAC.state_dict(),\n",
    "                    \"train_loss\":avg_train_loss,\n",
    "                    \"dev_acc\":avg_knn_auc,\n",
    "                }, hyper[\"checkpoint_path\"])\n",
    "                \n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch: \" + str(epoch) + \", acc: \" + str(avg_knn_auc))\n",
    "    test_knn_score, test_mse_score = model_eval(netRNA, netATAC, test_data_filtered, test_data_raw)\n",
    "    print(f\"test_knn_auc:{test_knn_score}\")\n",
    "    print(f\"test_mse:{test_mse_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train loss: 27.417362485613143\n",
      "Epoch: 0, acc: 0.06806408524840075\n",
      "Epoch: 5, train loss: 4.163197176797049\n",
      "Epoch: 5, acc: 0.0706868291976038\n",
      "Epoch: 10, train loss: 1.9315637690680367\n",
      "Epoch: 10, acc: 0.08422609039402526\n",
      "Epoch: 15, train loss: 1.4245105811527796\n",
      "Epoch: 15, acc: 0.08535714766104417\n",
      "Epoch: 20, train loss: 1.3089944124221802\n",
      "Epoch: 20, acc: 0.09128427065638255\n",
      "Epoch: 25, train loss: 1.1250764301845007\n",
      "Epoch: 25, acc: 0.10219473760983888\n",
      "Epoch: 30, train loss: 1.0826023306165422\n",
      "Epoch: 30, acc: 0.10291280570177945\n",
      "Epoch: 35, train loss: 1.0250601427895683\n",
      "Epoch: 35, acc: 0.10207132890990836\n",
      "Epoch: 40, train loss: 0.971937528678349\n",
      "Epoch: 40, acc: 0.09883173723480207\n",
      "Epoch: 45, train loss: 0.9465179358209882\n",
      "Epoch: 45, acc: 0.10354967189942794\n",
      "Epoch: 50, train loss: 0.8922159246035984\n",
      "Epoch: 50, acc: 0.10436373954199794\n",
      "Epoch: 55, train loss: 0.8888266171727862\n",
      "Epoch: 55, acc: 0.10375526663619591\n",
      "Epoch: 60, train loss: 0.8594572288649422\n",
      "Epoch: 60, acc: 0.10304756528597392\n",
      "Epoch: 65, train loss: 0.8575030394962856\n",
      "Epoch: 65, acc: 0.10776071108757415\n",
      "Epoch: 70, train loss: 0.8248958757945469\n",
      "Epoch: 70, acc: 0.10167965627717039\n",
      "Epoch: 75, train loss: 0.8318734339305333\n",
      "Epoch: 75, acc: 0.09967291235760654\n",
      "Epoch: 80, train loss: 0.7824629374912807\n",
      "Epoch: 80, acc: 0.10061579889890628\n",
      "Epoch: 85, train loss: 0.7828334059034076\n",
      "Epoch: 85, acc: 0.10014885006616969\n",
      "Epoch: 90, train loss: 0.7468071154185704\n",
      "Epoch: 90, acc: 0.0978698852862798\n",
      "Epoch: 95, train loss: 0.7682621393884931\n",
      "Epoch: 95, acc: 0.10060286232234744\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(netRNA, netATAC, test_data_filtered, test_data_raw, title):\n",
    "    netRNA.eval()\n",
    "    netATAC.eval()\n",
    "    rna_inputs = Variable(torch.from_numpy(test_data_filtered.X.toarray()).float())\n",
    "    atac_inputs = Variable(torch.from_numpy(test_data_filtered.obsm[\"mode2\"].toarray()).float())\n",
    "    if torch.cuda.is_available():\n",
    "        rna_inputs = rna_inputs.cuda()\n",
    "        atac_inputs = atac_inputs.cuda()\n",
    "    _, z_rna, _, _ = netRNA(rna_inputs)\n",
    "    _, z_atac, _, _ = netATAC(atac_inputs)\n",
    "    test_data_raw.obsm[\"aligned\"] = sparse.csr_matrix(z_rna.cpu().detach())\n",
    "    test_data_raw.obsm[\"mode2_aligned\"] = sparse.csr_matrix(z_atac.cpu().detach())\n",
    "    metrics.plot_multimodal_umap(test_data_raw, title=title, num_points=100, connect_modalities=True)\n",
    "    knn_score, mse_score = metrics.knn_auc(test_data_raw), metrics.mse(test_data_raw)\n",
    "    return knn_score, mse_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load checkpoint\n",
    "checkpoint=None\n",
    "if path.exists(hyper[\"checkpoint_path\"]):\n",
    "    checkpoint = torch.load(hyper[\"checkpoint_path\"], map_location=\"cpu\")\n",
    "netRNA = FC_VAE(n_input=hyper[\"dimRNA\"], nz=hyper[\"nz\"], layer_sizes=hyper[\"layer_sizes\"])\n",
    "netATAC = FC_VAE(n_input=hyper[\"dimATAC\"], nz=hyper[\"nz\"], layer_sizes=hyper[\"layer_sizes\"])\n",
    "if checkpoint != None:\n",
    "    netRNA.load_state_dict(checkpoint[\"net_rna_state_dict\"])\n",
    "    netATAC.load_state_dict(checkpoint[\"net_atac_state_dict\"])\n",
    "    \n",
    "#plot UMAP result and show evaluation metrics value\n",
    "model_eval(netRNA, netATAC, test_data_filtered, test_data_raw, title=\"VAE with Structure-Preserving Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similairty(netRNA, netATAC, test_data_filtered):\n",
    "    netRNA.eval()\n",
    "    netATAC.eval()\n",
    "    rna_inputs = Variable(torch.from_numpy(test_data_filtered.X.toarray()).float())\n",
    "    atac_inputs = Variable(torch.from_numpy(test_data_filtered.obsm[\"mode2\"].toarray()).float())\n",
    "    if torch.cuda.is_available():\n",
    "        rna_inputs = rna_inputs.cuda()\n",
    "        atac_inputs = atac_inputs.cuda()\n",
    "    _, z_rna, _, _ = netRNA(rna_inputs)\n",
    "    _, z_atac, _, _ = netATAC(atac_inputs)\n",
    "    cos_score = nn.CosineSimilairty(z_rna, z_atac)\n",
    "    return(cos_score)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "autoencoder-scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
