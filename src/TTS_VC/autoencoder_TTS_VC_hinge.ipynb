{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc5d32c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 2386, 49])\n"
     ]
    }
   ],
   "source": [
    "##### -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata\n",
    "import scprep\n",
    "import scanpy as sc\n",
    "import sklearn\n",
    "import tempfile\n",
    "from os import path\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from cca_loss import cca_loss\n",
    "from FC_VAE import FC_VAE\n",
    "\n",
    "\n",
    "def load_binary_file(file_name, dimension):\n",
    "    fid_lab = open(file_name, 'rb')\n",
    "    features = np.fromfile(fid_lab, dtype=np.float32)\n",
    "    fid_lab.close()\n",
    "    assert features.size % float(dimension) == 0.0, 'specified dimension %s not compatible with data' % (dimension)\n",
    "    features = features[:(dimension * (features.size // dimension))]\n",
    "    features = features.reshape((-1, dimension))\n",
    "    return features\n",
    "\n",
    "def load_binary_file_frame(self, file_name, dimension):\n",
    "    fid_lab = open(file_name, 'rb')\n",
    "    features = numpy.fromfile(fid_lab, dtype=numpy.float32)\n",
    "    fid_lab.close()\n",
    "    assert features.size % float(dimension) == 0.0, 'specified dimension %s not compatible with data' % (dimension)\n",
    "    frame_number = features.size // dimension\n",
    "    features = features[:(dimension * frame_number)]\n",
    "    features = features.reshape((-1, dimension))\n",
    "\n",
    "    return features, frame_number\n",
    "\n",
    "\n",
    "\n",
    "dir_phonePath = \"/home/rania/Documents/workspace/tools/merlin/output\"\n",
    "dir_PPGpath = \"/home/rania/Documents/workspace/IntraSpkVC/data_fr/ppg/FFR0009\"\n",
    "phone_data = []\n",
    "PPG_data = []\n",
    "\n",
    "PPG_lengths = []\n",
    "phone_lengths = []\n",
    "\n",
    "\n",
    "def load_data(dir_path, x):\n",
    "    \n",
    "    file_list = glob.glob(dir_path + '/*.'+x+'')\n",
    "    \n",
    "    for idx, s in enumerate(file_list):\n",
    "        if x == 'flab':\n",
    "            data =torch.tensor(load_binary_file(s, 49))\n",
    "#             print(data.shape)\n",
    "#           print(len(data[:,0]))\n",
    "            phone_data.append(data)\n",
    "            phone_lengths.append(len(data))\n",
    "        else:\n",
    "            data = np.load(s)\n",
    "            PPG_data.append(data)\n",
    "            PPG_lengths.append(len(data))\n",
    "\n",
    "    return phone_data, phone_lengths, PPG_data, PPG_lengths\n",
    "\n",
    "phone_data, phone_lengths, _, _ = load_data(dir_phonePath, 'flab')\n",
    "# print(\"phone_data {} {}\".format(  len(phone_data), len(phone_data[1])  )    )\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.data = pad_sequence(data, batch_first=True, padding_value=0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx].clone().detach().requires_grad_(True)\n",
    "        x = torch.unsqueeze(x, 0)\n",
    "        return x  \n",
    "\n",
    "# _, _, PPG_data, PPG_lengths = load_data(dir_phonePath, 'npy')\n",
    "\n",
    "padded_phone = Dataset(phone_data)\n",
    "iterator = iter(padded_phone)\n",
    "\n",
    "for i, item in enumerate(train_loader):\n",
    "    print(item.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88101cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper = {\n",
    "    \"nEpochs\":100,\n",
    "    \"dim_rna\":2386,\n",
    "    \"dim_atac\":2386,\n",
    "    \"n_latent\":32,\n",
    "    \"layer_sizes\":[256, 64],\n",
    "    \"add_hinge\":False,\n",
    "    \"lamb_match\":1,\n",
    "    \"lamb_nn\":1.5,\n",
    "    \"lamb_hinge\":10,\n",
    "    \"train_batch\":256,\n",
    "    \"test_batch\": 512,\n",
    "    \"lr\": 1e-3,\n",
    "    \"clip_grad\": 1,\n",
    "    \"checkpoint_path\": './checkpoint/best_model_dcca_hinge.pt',\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "793886ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_loss(x, x_recon, y, y_recon):\n",
    "    MSE = nn.MSELoss()\n",
    "    x_loss = MSE(x, x_recon)\n",
    "    y_loss = MSE(y, y_recon)\n",
    "    total_loss = x_loss + y_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43cfefdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_input, n_latent, layer_sizes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.n_latent = n_latent\n",
    "        self.layer_sizes = [n_input] + layer_sizes + [n_latent]\n",
    "        self.encoder_layers = []\n",
    "        \n",
    "        for idx in range(len(self.layer_sizes) - 1):\n",
    "            fc1 = nn.Linear(self.layer_sizes[idx], self.layer_sizes[idx + 1])\n",
    "            nn.init.xavier_uniform_(fc1.weight)\n",
    "            self.encoder_layers.append(fc1)\n",
    "            bn1 = nn.BatchNorm1d(self.layer_sizes[idx + 1])\n",
    "            self.encoder_layers.append(bn1)\n",
    "            act1 = nn.PReLU()\n",
    "            self.encoder_layers.append(act1)\n",
    "            \n",
    "        self.encoder = nn.Sequential(*self.encoder_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c4823cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCCA(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_input1, \n",
    "        n_input2, \n",
    "        layer_sizes1, \n",
    "        layer_sizes2, \n",
    "        n_out, \n",
    "        use_all_singvals=False, \n",
    "        device=torch.device(\"cpu\"), \n",
    "        use_decode = False, \n",
    "        seed=182822,\n",
    "    ):\n",
    "        super(DCCA, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.Net1 = Encoder(n_input1, n_latent=n_out, layer_sizes=layer_sizes1)\n",
    "        self.Net2 = Encoder(n_input2, n_latent=n_out, layer_sizes=layer_sizes2)\n",
    "        self.cca_loss = cca_loss(out_dim=n_out, use_all_singvals=use_all_singvals, device=device).loss\n",
    "        self.recon_loss = recon_loss\n",
    "        \n",
    "        self.device = device\n",
    "        self.use_decode = use_decode\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        z1 = self.Net1(x1)\n",
    "        embed1 = F.normalize(z1, dim=1, p=2)\n",
    "        z2 = self.Net2(x2)\n",
    "        embed2 = F.normalize(z2, dim=1, p=2)\n",
    "        return embed1, embed2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "528fb719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(train_data_filtered, test_data_filtered, train_data_raw, test_data_raw):\n",
    "    train_set = trainDataX\n",
    "    test_set = valDataX\n",
    "    \n",
    "    #load data loader\n",
    "    train_loader = DataLoader(\n",
    "        train_set, \n",
    "        batch_size=hyper[\"train_batch\"], \n",
    "        drop_last=False, \n",
    "        shuffle=True,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_set, \n",
    "        batch_size=hyper[\"test_batch\"], \n",
    "        drop_last=False, \n",
    "        shuffle=False,\n",
    "    )\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b42f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f33443cbc10> <torch.utils.data.dataloader.DataLoader object at 0x7f33443cb130>\n",
      "torch.Size([32, 1, 2386, 49])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (76352x49 and 2386x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_190848/3957843423.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mphone_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphone_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0membed_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdcca_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphone_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphone_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m#calculate train loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_190848/1873405748.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mz1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNet1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0membed1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mz2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNet2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_190848/4286457878.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (76352x49 and 2386x256)"
     ]
    }
   ],
   "source": [
    "trainDataX, valDataX,trainDataY, valDataY = train_test_split([data for data in padded_phone ],[data for data in padded_phone ],test_size=0.2,random_state=41,shuffle=False,stratify=None)\n",
    "# print(\"size of the training dataset {}\".format(len(trainDataX)), trainDataX[2].shape)\n",
    "\n",
    "train_loader, test_loader = get_data_loaders(\n",
    "        trainDataX, \n",
    "        valDataX,\n",
    "        trainDataX,\n",
    "        valDataX,\n",
    "    )\n",
    "\n",
    "print(train_loader, test_loader)\n",
    "for i, item in enumerate(train_loader):\n",
    "    print(item.shape)\n",
    "\n",
    "#load checkpoint\n",
    "checkpoint = None\n",
    "if path.exists(hyper[\"checkpoint_path\"]):\n",
    "    checkpoint = torch.load(hyper[\"checkpoint_path\"])\n",
    "\n",
    "my_device = 'cpu'    \n",
    "#load basic models\n",
    "dcca_model = DCCA(\n",
    "    n_input1=hyper[\"dim_rna\"], \n",
    "    n_input2=hyper[\"dim_atac\"], \n",
    "    layer_sizes1=hyper[\"layer_sizes\"], \n",
    "    layer_sizes2=hyper[\"layer_sizes\"], \n",
    "    n_out=hyper[\"n_latent\"],\n",
    "    use_all_singvals=False,\n",
    "    device=torch.device(my_device), \n",
    "    use_decode = False,\n",
    ")\n",
    "\n",
    "\n",
    "if checkpoint != None:\n",
    "    dcca_model.load_state_dict(checkpoint[\"netPhone_state_dict\"])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"using GPU\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "dcca_model.to(device)\n",
    "\n",
    "#set up optimizer\n",
    "dcca_opt = optim.Adam(list(dcca_model.parameters()), lr=hyper[\"lr\"])\n",
    "dcca_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    dcca_opt, \n",
    "    patience=10,\n",
    "    factor=0.5,\n",
    "    threshold=0.3, \n",
    "    threshold_mode=\"abs\",\n",
    "    min_lr=1e-5,\n",
    ")\n",
    "\n",
    "#set up device \n",
    "device=hyper[\"device\"]\n",
    "dcca_model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "#training step\n",
    "for epoch in range(hyper[\"nEpochs\"]):\n",
    "    train_losses = []\n",
    "    for idx, (phone_data) in enumerate(train_loader):\n",
    "#         phone_data.zero_grad()\n",
    "\n",
    "        phone_data = Variable(phone_data).to(device)\n",
    "        \n",
    "        embed_1, embed_2 = dcca_model(phone_data, phone_data)\n",
    "\n",
    "        #calculate train loss\n",
    "        train_loss = dcca_model.cca_loss(embed_1, embed_2)\n",
    "        if hyper[\"add_hinge\"]:\n",
    "            hinge_loss = StructureHingeLoss(\n",
    "                margin=0.25, \n",
    "                max_val=1e6, \n",
    "                lamb_match=hyper[\"lamb_match\"], \n",
    "                lamb_nn=hyper[\"lamb_nn\"],\n",
    "                device=device,\n",
    "            )\n",
    "            h_loss = hinge_loss(embed_rna, embed_atac, nn_indices)\n",
    "            #train_loss = train_loss + h_loss\n",
    "            train_loss += hyper['lamb_hinge'] * h_loss\n",
    "            #print(f\"train loss: {train_loss}\")\n",
    "            #print(f\"hinge loss: {h_loss}\")\n",
    "            #print(f\"train loss with hinge: {train_loss}\")\n",
    "        train_loss.backward()\n",
    "        #nn.utils.clip_grad_norm_(toy_model.parameters(), max_norm=hyper[\"clip_grad\"])\n",
    "        dcca_opt.step()\n",
    "        dcca_scheduler.step(train_loss)\n",
    "        train_losses.append(train_loss.item())\n",
    "    avg_train_loss = np.mean(train_losses)\n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Epoch: \" + str(epoch) + \", train loss: \" + str(avg_train_loss))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
